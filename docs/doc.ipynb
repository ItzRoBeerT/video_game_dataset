{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentacion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotros hemos elegido dos datasets sobre videojuegos, uno de ellos consta de la informacion de ventas de estos juegos mientras que los otros de la ubicacion de las empresas que los han creado. \n",
    "\n",
    "Como estos dos datasets eran del formato csv, hemos tenido que convertir uno de ellos a formato parquet. Para ellos hemos usado el siguiente codigo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"your_file.csv\"\n",
    "parquet_file = \"your_file.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df.to_parquet(parquet_file, engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto ya tendriamos guardado el dataset como formato parquet en el directorio en el que estamos. El siguiente paso seria el tratamiento de datos, donde vemos que datos son los que queremos mantener para luego juntar los datasets. \n",
    "\n",
    "Para ello nos dividimos los datasets, tratando uno cada uno, eliminado columnas innecesarias como `Est.` (El año en el que se establecio la empresa) que no son importantes para el objetivo que buscamos. Una vez hemos filtrado las columnas que nos interesan ahora queda tratar los valores nulos, en los que tenemos varias formas de elimianrlos, la que nosoros hemos usado es eliminar las filas con nulos, ya que al tener uan gran cantidad de datos y ser pocas filas podemos permitirnos perder algunas filas. \n",
    "\n",
    "Ahora con los datasets ya filtrados, podemos pasar a juntarlos, lo cual es algo simple. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_games = pd.concat([indie_games_csv, video_games_csv], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta funcion de pandas unimos los dos datasets, aunque, ahora nos surge otro problema, las filas que estan en un dataset pero no en el otro tienen valores nulos, los cuales debemos tratar de alguna forma. Ademas nos surgen algunos duplicados los cuales podemos eliminar con este codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_games = video_games.drop_duplicates(subset='Developer', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el unico problema que nos queda es con los valores nulos que surgen al mezclar este dataset, (TO BE CONTINUED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face [Link](https://huggingface.co/datasets/ItzRoBeerT/video-games-sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir nuestro dataset a hugging face\n",
    "Ya tenemos nuestro dataframe listo para subir a hugging face. \n",
    "\n",
    "Ahora sólo nos hace falta:\n",
    "- Nuestro token de hugging (Disponible en access tokens)\n",
    "- tener importado la librería `huggingface_hub` del cual usaremos la función `login`\n",
    "- Importar la clase `Dataset`de la librería `datasets`\n",
    "\n",
    "### Pasos a seguir:\n",
    "\n",
    "1. Primero importamos las librerías que utilizaremos:\n",
    "```python\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import  login\n",
    "```\n",
    "2. Obtener nuestro token de acceso de hugging face:\n",
    "\n",
    "Se puede obtener cuando hacemos click en  nuestro perfil -> access token -> create new token\n",
    "\n",
    "!Importante conceder permisos de **escritura**¡\n",
    "\n",
    "_OJO_\n",
    "En este caso, hemos usado la función `getenv` de la librería `os` para tener nuestro token de acceso en una variable de entorno y que de esta forma, no sea accesible públicamente.\n",
    "\n",
    "Sólo tenemos que llamar a la función `getenv` y pasarle el nombre que hemos especificado en nuestro archivo .env\n",
    "```python\n",
    "\n",
    "token = getenv('VARIABLE_CON_EL_TOKEN')\n",
    "``` \n",
    "\n",
    "3. Parseamos nuestro dataframe a dataset \n",
    "\n",
    "Gracias a la clase `Dataset`, ya trae una función `from_pandas` la cual convierte nuestro dataframe en un dataset listo para publicarlo a nuestra cuenta de **huggingn face**\n",
    "\n",
    "4. Loguearnos en hugging face\n",
    "\n",
    "```python\n",
    "login(token=token)\n",
    "```\n",
    "\n",
    "5. Publicar el dataset\n",
    "\n",
    "```python\n",
    "dataset.push_to_hub('nombre-del-repo')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aitor Mendrago [Link](https://aitor-medrano.github.io/iabd/hf/datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un dataset a partir de nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más comun de crear un dataset a partir de nuestros propios datos, es crear un diccionario o una lista y crear el dataset a partir de ello. Luego podemos usar `Dataset.from_dict()` o `Dataset.from_list()` para crear nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset desde diccionario:\n",
      "Dataset({\n",
      "    features: ['texto', 'etiqueta', 'id'],\n",
      "    num_rows: 3\n",
      "})\n",
      "\n",
      "Primera fila: {'texto': 'Hola mundo', 'etiqueta': 0, 'id': 1}\n",
      "\n",
      "Dataset desde lista:\n",
      "Dataset({\n",
      "    features: ['texto', 'etiqueta', 'id'],\n",
      "    num_rows: 3\n",
      "})\n",
      "\n",
      "Primera fila: {'texto': 'Hola mundo', 'etiqueta': 0, 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "# Primero instala la biblioteca si no la tienes\n",
    "# pip install datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Datos de ejemplo en formato diccionario (columnas)\n",
    "datos_dict = {\n",
    "    \"texto\": [\"Hola mundo\", \"Python es genial\", \"Aprendiendo NLP\"],\n",
    "    \"etiqueta\": [0, 1, 0],\n",
    "    \"id\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Crear dataset desde diccionario\n",
    "dataset_from_dict = Dataset.from_dict(datos_dict)\n",
    "\n",
    "# Datos de ejemplo en formato lista (filas)\n",
    "datos_list = [\n",
    "    {\"texto\": \"Hola mundo\", \"etiqueta\": 0, \"id\": 1},\n",
    "    {\"texto\": \"Python es genial\", \"etiqueta\": 1, \"id\": 2},\n",
    "    {\"texto\": \"Aprendiendo NLP\", \"etiqueta\": 0, \"id\": 3}\n",
    "]\n",
    "\n",
    "# Crear dataset desde lista\n",
    "dataset_from_list = Dataset.from_list(datos_list)\n",
    "\n",
    "# Mostrar estructura del dataset\n",
    "print(\"Dataset desde diccionario:\")\n",
    "print(dataset_from_dict)\n",
    "print(\"\\nPrimera fila:\", dataset_from_dict[0])\n",
    "\n",
    "print(\"\\nDataset desde lista:\")\n",
    "print(dataset_from_list)\n",
    "print(\"\\nPrimera fila:\", dataset_from_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trabajar con filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo que solemos usar más es el `Dataset.filter()` ya que nos permite buscar en el dataset en base a una condicion que le demos, aunque si queremos seleccionar solamente algunas filas podemos usar el metodo `Dataset.select()` para ello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.', \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\", 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.', 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.'], 'label': [2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar un dataset de ejemplo\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "\n",
    "# Asegurarse de que la categoría (label) sea un entero\n",
    "dataset = dataset.map(lambda example: {\"label\": int(example[\"label\"])})\n",
    "\n",
    "# Filtrar solo las filas donde la categoría (label) sea 2\n",
    "filtered_dataset = dataset.filter(lambda example: example[\"label\"] == 2)\n",
    "\n",
    "# Seleccionar solo las columnas \"text\" y \"label\"\n",
    "selected_dataset = filtered_dataset.remove_columns([col for col in filtered_dataset.column_names if col not in [\"text\", \"label\"]])\n",
    "\n",
    "# Mostrar algunas filas del dataset resultante\n",
    "print(selected_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trabajar con columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las columnas tenemos varias funciones, `Dataset.add_column()`, `Dataset.rename_column()` y `Dataset.remove_column()`, cada una hace lo que puedes esperar, añadir, renombrar y eliminar columnas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra funcion a destacar es la funcion `Dataset.map()`, la cual nos permite ejecutar una funcion en el dataset y si modificamos alguna columna del dataset que ya existe la sobreescribe. Esta funcion map() es la que usamos para tratar los datos del dataset, junto con la funcion `Dataset.sort()`, la cual es muy util para visualizar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset inicial: Dataset({\n",
      "    features: ['texto', 'etiqueta', 'id'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f68e3c5c594d0daca8429366758d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5123a130f04825941579ff202efab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset procesado:\n",
      "Dataset({\n",
      "    features: ['texto', 'label', 'longitud', 'texto_upper', 'es_positivo', 'palabras'],\n",
      "    num_rows: 4\n",
      "})\n",
      "\n",
      "Ejemplos finales:\n",
      "texto\n",
      "label\n",
      "longitud\n",
      "texto_upper\n",
      "es_positivo\n",
      "palabras\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Dataset inicial\n",
    "datos = {\n",
    "    \"texto\": [\"Transformers son poderosos\", \"Python vs Java\", \"IA revoluciona el mundo\", \"Hola\"],\n",
    "    \"etiqueta\": [1, 0, 1, 0],\n",
    "    \"id\": [101, 102, 103, 104]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(datos)\n",
    "\n",
    "# Función para procesamiento con map\n",
    "def procesar_ejemplo(ejemplo):\n",
    "    # Calcular longitud del texto\n",
    "    longitud = len(ejemplo[\"texto\"])\n",
    "    # Convertir texto a mayúsculas\n",
    "    texto_mayus = ejemplo[\"texto\"].upper()\n",
    "    # Determinar si es positivo (etiqueta 1)\n",
    "    es_positivo = ejemplo[\"etiqueta\"] == 1\n",
    "    return {\n",
    "        \"longitud\": longitud,\n",
    "        \"texto_upper\": texto_mayus,\n",
    "        \"es_positivo\": es_positivo\n",
    "    }\n",
    "\n",
    "print(\"\\nDataset inicial:\", dataset)\n",
    "\n",
    "# Aplicar map (agregando nuevas columnas)\n",
    "dataset = dataset.map(procesar_ejemplo)\n",
    "\n",
    "# Ordenar por longitud descendente\n",
    "dataset = dataset.sort(\"longitud\", reverse=True)\n",
    "\n",
    "# Operaciones con columnas\n",
    "# Renombrar columna\n",
    "dataset = dataset.rename_column(\"etiqueta\", \"label\")\n",
    "\n",
    "# Agregar nueva columna calculada\n",
    "dataset = dataset.add_column(\"palabras\", [len(text.split()) for text in dataset[\"texto\"]])\n",
    "\n",
    "# Eliminar columna\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "\n",
    "# Mostrar estructura final\n",
    "print(\"\\nDataset procesado:\", dataset)\n",
    "\n",
    "# Mostrar primeros 2 ejemplos\n",
    "print(\"\\nEjemplos finales:\")\n",
    "for ejemplo in dataset[:2]:\n",
    "    print(ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion map(), nos permite trabajar tambien con conjuntos de datos, seleccionando el parametro *batched* a True, de esta manera los datos se procesaran por conjuntos, es decir generando listas completas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Dataset con 3 textos\n",
    "dataset = Dataset.from_dict({\n",
    "    \"texto\": [\"Hola\", \"Python es divertido\", \"Inteligencia Artificial\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e26370d1384b54863fdf71d4a09fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada de la funcion: {'texto': 'Hola'}\n",
      "Entrada de la funcion: {'texto': 'Python es divertido'}\n",
      "Entrada de la funcion: {'texto': 'Inteligencia Artificial'}\n",
      "Sin batched (ejemplo por ejemplo):\n",
      "{'texto': 'Hola', 'longitud': 4, 'mayusculas': 'HOLA'}\n"
     ]
    }
   ],
   "source": [
    "# Función que procesa UN solo ejemplo\n",
    "def procesar_individual(ejemplo):\n",
    "    print(\"Entrada de la funcion:\", ejemplo)\n",
    "    return {\n",
    "        \"longitud\": len(ejemplo[\"texto\"]),\n",
    "        \"mayusculas\": ejemplo[\"texto\"].upper()\n",
    "    }\n",
    "\n",
    "# Aplicar map (sin batched)\n",
    "dataset_sin_batch = dataset.map(procesar_individual)\n",
    "\n",
    "print(\"Sin batched (ejemplo por ejemplo):\")\n",
    "print(dataset_sin_batch[0])  # Primer ejemplo procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f59c4220e741e2b3aedbbfb4a90954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada de la funcion {'texto': ['Hola', 'Python es divertido', 'Inteligencia Artificial']}\n",
      "\n",
      "Con batched (procesamiento por lotes):\n",
      "{'texto': 'Hola', 'longitud': 4, 'mayusculas': 'HOLA'}\n"
     ]
    }
   ],
   "source": [
    "# Función que procesa un conjunto de ejemplos\n",
    "def procesar_lote(lote):\n",
    "    print(\"Entrada de la funcion\", lote)\n",
    "    return {\n",
    "        \"longitud\": [len(t) for t in lote[\"texto\"]],\n",
    "        \"mayusculas\": [t.upper() for t in lote[\"texto\"]]\n",
    "    }\n",
    "\n",
    "# Aplicar map (con batched=True)\n",
    "dataset_con_batch = dataset.map(procesar_lote, batched=True)\n",
    "\n",
    "print(\"\\nCon batched (procesamiento por lotes):\")\n",
    "print(dataset_con_batch[0])  # Mismo resultado, pero procesado en bloque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestionar espacio de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos datasets muy grandes, como por ejemplo BigCode que ocupa 67.5 TB,  que no podemos ejecutar en RAM podemos usar el parametro streaming de Dataset.load_dataset(), con este parametro accedemos a los datos por fragmenos, sobre los que podremos ir iterando usando la funcion next(), o con un bucle tradicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375040f955a64824918b3b1e55cf14a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 3 ejemplos del dataset (streaming):\n",
      "\n",
      "Ejemplo 1:\n",
      "Texto: Wall St. Bears Claw Back Into the Black (Reuters) ...\n",
      "Etiqueta: 2\n",
      "\n",
      "Ejemplo 2:\n",
      "Texto: Carlyle Looks Toward Commercial Aerospace (Reuters...\n",
      "Etiqueta: 2\n",
      "\n",
      "Ejemplo 3:\n",
      "Texto: Oil and Economy Cloud Stocks' Outlook (Reuters) Re...\n",
      "Etiqueta: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar dataset en modo streaming (aunque sea pequeño)\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\", streaming=True)\n",
    "\n",
    "print(\"Primeros 3 ejemplos del dataset (streaming):\")\n",
    "for i, ejemplo in enumerate(dataset):\n",
    "    if i >= 3:  # Limitar a 3 ejemplos para demostración\n",
    "        break\n",
    "    print(f\"\\nEjemplo {i + 1}:\")\n",
    "    print(f\"Texto: {ejemplo['text'][:50]}...\")  \n",
    "    print(f\"Etiqueta: {ejemplo['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subir el dataset a Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer el login en hugging face lo podemos hacer desde la terminal con `huggingface-cli login`, o con la funcion login() de hugging face hub. \n",
    "\n",
    "Para subir el datset podemos usar la funcion `push_to_hub`, y podemos cambiar el parametro private si querems subirlo en privado. Tambien tenemos el parametro split, por si tenemos datasets de train, y de validacion  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31df39da8b94c61b48f470191fd8874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# dentro de la funcion login metemos nuestro token de hugging face\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = Dataset.from_dict({\n",
    "    \"texto\": [\"Hola\", \"Python es divertido\", \"Inteligencia Artificial\"]\n",
    "})\n",
    "\n",
    "sample_dataset.push_to_hub(\"user/repo_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el enlace al dataset subido: [Enlace](https://huggingface.co/datasets/JuanjoJ55/video-games-sales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
